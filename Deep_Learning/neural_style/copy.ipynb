{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_img = Image.open('final_style.png')\n",
    "style_img = Image.open('final_content.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing\n",
    "\n",
    "rgb_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "rgb_std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "def preprocess(img,image_shape):\n",
    "    transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(image_shape),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=rgb_mean,std=rgb_std)\n",
    "    ])\n",
    "    return transforms(img).unsqueeze(0)\n",
    "\n",
    "def postprocess(img):\n",
    "    img = img[0].to(rgb_std.device)\n",
    "    img = torch.clamp(img.permute(1,2,0)*rgb_std+rgb_mean,0,1)\n",
    "    return torchvision.transforms.ToPILImage()(img.permute(2,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lucent.modelzoo.inceptionv1 import InceptionV1\n",
    "from lucent.misc.io import show\n",
    "\n",
    "import lucent.optvis.objectives as objectives\n",
    "import lucent.optvis.param as param\n",
    "import lucent.optvis.render as render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionV1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_layers = [\n",
    "  'conv2d2',\n",
    "  'mixed3a',\n",
    "  'mixed4a',\n",
    "  'mixed4b',\n",
    "  'mixed4c',\n",
    "]\n",
    "\n",
    "content_layers = [\n",
    "  'mixed3b',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape =  (250, 325) \n",
    "\n",
    "def style_transfer_param(content_image, style_image, decorrelate=True, fft=True):\n",
    "    \n",
    "    style_transfer_input = param.image(*preprocess(content_image,image_shape).shape[:2], decorrelate=decorrelate, fft=fft)[0]\n",
    "    content_input = content_image\n",
    "#     print(style_transfer_input)\n",
    "    return style_transfer_input, content_input, style_image\n",
    "\n",
    "# these constants help remember which image is at which batch dimension\n",
    "TRANSFER_INDEX = 0\n",
    "CONTENT_INDEX = 1\n",
    "STYLE_INDEX = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([[[[[ 0.0051,  0.0295]],\n",
       "  \n",
       "            [[-0.0161, -0.0074]],\n",
       "  \n",
       "            [[-0.0209,  0.0009]]],\n",
       "  \n",
       "  \n",
       "           [[[-0.0036, -0.0135]],\n",
       "  \n",
       "            [[ 0.0133,  0.0056]],\n",
       "  \n",
       "            [[ 0.0128,  0.0058]]],\n",
       "  \n",
       "  \n",
       "           [[[ 0.0051, -0.0207]],\n",
       "  \n",
       "            [[-0.0102,  0.0028]],\n",
       "  \n",
       "            [[-0.0168, -0.0018]]]]], device='cuda:0', requires_grad=True)],\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=RGB size=512x512 at 0x22167C8DA88>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=RGB size=645x512 at 0x22167C8D448>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style_transfer_param(content_img, style_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_L1(a, b):\n",
    "    return torch.mean(torch.abs(a-b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_difference(layer_names, activation_loss_f=mean_L1, transform_f=None, difference_to=CONTENT_INDEX):\n",
    "    def inner(T):\n",
    "        # first we collect the (constant) activations of image we're computing the difference to\n",
    "        image_activations = [T(layer_name)[difference_to] for layer_name in layer_names]\n",
    "        if transform_f is not None:\n",
    "            image_activations = [transform_f(act) for act in image_activations]\n",
    "\n",
    "        # we also set get the activations of the optimized image which will change during optimization\n",
    "        optimization_activations = [T(layer)[TRANSFER_INDEX] for layer in layer_names]\n",
    "        if transform_f is not None:\n",
    "            optimization_activations = [transform_f(act) for act in optimization_activations]\n",
    "\n",
    "        # we use the supplied loss function to compute the actual losses\n",
    "        losses = [activation_loss_f(a, b) for a, b in zip(image_activations, optimization_activations)]\n",
    "        return tf.add_n(losses)\n",
    "\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gram_matrix(array, normalize_magnitue=True):\n",
    "#     channels = tf.shape(array)[-1]\n",
    "#     array_flat = tf.reshape(array, [-1, channels])\n",
    "#     gram_matrix = tf.matmul(array_flat, array_flat, transpose_a=True)\n",
    "#     if normalize_magnitue:\n",
    "#     length = tf.shape(array_flat)[0]\n",
    "#     gram_matrix /= tf.cast(length, tf.float32)\n",
    "#     return gram_matrix\n",
    "\n",
    "def gram_matrix(X):\n",
    "    num_channels, n = X.shape[1], X.numel() // X.shape[1]\n",
    "    X = X.reshape((num_channels, n))\n",
    "    return torch.matmul(X, X.T) / (num_channels * n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'int' and 'function'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-00eaf867df07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mparam_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstyle_transfer_param\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstyle_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcontent_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mactivation_difference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdifference_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCONTENT_INDEX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mcontent_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescription\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Content Loss\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'int' and 'function'"
     ]
    }
   ],
   "source": [
    "param_f = lambda: style_transfer_param(content_image, style_image)\n",
    "\n",
    "content_obj = 100 * activation_difference(content_layers, difference_to=CONTENT_INDEX)\n",
    "content_obj.description = \"Content Loss\"\n",
    "\n",
    "style_obj = activation_difference(style_layers, transform_f=gram_matrix, difference_to=STYLE_INDEX)\n",
    "style_obj.description = \"Style Loss\"\n",
    "\n",
    "objective = - content_obj - style_obj\n",
    "\n",
    "vis = render.render_vis(model, objective, param_f=param_f, thresholds=[512], verbose=False, print_objectives=[content_obj, style_obj])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
