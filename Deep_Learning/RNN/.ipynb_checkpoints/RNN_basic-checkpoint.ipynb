{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import zipfile\n",
    "import hashlib\n",
    "import collections\n",
    "import random\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = dict()\n",
    "url = 'http://d2l-data.s3-accelerate.amazonaws.com/'\n",
    "Data['time_machine'] = (url + 'timemachine.txt',\n",
    "                                '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
    "\n",
    "def download(name ,cache_dir=os.path.join('..', 'data')):\n",
    "    assert name in Data, f\"{name} does not exist in {Data}.\"\n",
    "    url, sha1_hash = Data[name]\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
    "    if os.path.exists(fname):\n",
    "        sha1 = hashlib.sha1()\n",
    "        with open(fname, 'rb') as f:\n",
    "            while True:\n",
    "                data = f.read(1048576)\n",
    "                if not data:\n",
    "                    break\n",
    "                sha1.update(data)\n",
    "        if sha1.hexdigest() == sha1_hash:\n",
    "            return fname  # Hit cache\n",
    "    print(f'Downloading {fname} from {url}...')\n",
    "    r = requests.get(url, stream=True, verify=True)\n",
    "    with open(fname, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    return fname\n",
    "\n",
    "def read_time_machine():  #@save\n",
    "    \"\"\"Load the time machine dataset into a list of text lines.\"\"\"\n",
    "    with open(download('time_machine'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
    "# ignoring punctuation and capitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# text lines: 3221\n",
      "twinkled and his usually pale face was flushed and animated the\n",
      "fire burned brightly and the soft radiance of the incandescent\n"
     ]
    }
   ],
   "source": [
    "lines = read_time_machine()\n",
    "print(f'# text lines: {len(lines)}')\n",
    "print(lines[10])\n",
    "print(lines[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Got Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lines, token='word'):\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('ERROR: unknown token type: ' + token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['i']\n",
      "[]\n",
      "[]\n",
      "['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n",
      "['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n",
      "['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenize(lines)\n",
    "for i in range(11):\n",
    "    print(tokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:  #@save\n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        # Sort according to frequencies\n",
    "        counter = count_corpus(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                  reverse=True)\n",
    "        # The index for the unknown token is 0\n",
    "        self.unk, uniq_tokens = 0, ['<unk>'] + reserved_tokens\n",
    "        uniq_tokens += [token for token, freq in self.token_freqs\n",
    "                        if freq >= min_freq and token not in uniq_tokens]\n",
    "        self.idx_to_token, self.token_to_idx = [], dict()\n",
    "        for token in uniq_tokens:\n",
    "            self.idx_to_token.append(token)\n",
    "            self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "def count_corpus(tokens):  #@save\n",
    "    \"\"\"Count token frequencies.\"\"\"\n",
    "    # Here `tokens` is a 1D list or 2D list\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # Flatten a list of token lists into a list of tokens\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<unk>', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(tokens)\n",
    "print(list(vocab.token_to_idx.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "indices: [1, 19, 50, 40, 2183, 2184, 400]\n",
      "words: ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n",
      "indices: [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]\n"
     ]
    }
   ],
   "source": [
    "# each text line into a list of numerical indices.\n",
    "for i in [0, 10]:\n",
    "    print('words:', tokens[i])\n",
    "    print('indices:', vocab[tokens[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170580, 28)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_corpus_time_machine(max_tokens=-1):  #@save\n",
    "    \"\"\"Return token indices and the vocabulary of the time machine dataset.\"\"\"\n",
    "    lines = read_time_machine()\n",
    "    tokens = tokenize(lines, 'char')\n",
    "    vocab = Vocab(tokens)\n",
    "    # Since each text line in the time machine dataset is not necessarily a\n",
    "    # sentence or a paragraph, flatten all the text lines into a single list\n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocab\n",
    "\n",
    "corpus, vocab = load_corpus_time_machine()\n",
    "len(corpus), len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_data_iter_random(corpus, batch_size, num_steps):  #@save\n",
    "    \"\"\"Generate a minibatch of subsequences using random sampling.\"\"\"\n",
    "    # Start with a random offset (inclusive of `num_steps - 1`) to partition a\n",
    "    # sequence\n",
    "    corpus = corpus[random.randint(0, num_steps - 1):]\n",
    "    # Subtract 1 since we need to account for labels\n",
    "    num_subseqs = (len(corpus) - 1) // num_steps\n",
    "    # The starting indices for subsequences of length `num_steps`\n",
    "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
    "    # In random sampling, the subsequences from two adjacent random\n",
    "    # minibatches during iteration are not necessarily adjacent on the\n",
    "    # original sequence\n",
    "    random.shuffle(initial_indices)\n",
    "\n",
    "    def data(pos):\n",
    "        # Return a sequence of length `num_steps` starting from `pos`\n",
    "        return corpus[pos: pos + num_steps]\n",
    "\n",
    "    num_batches = num_subseqs // batch_size\n",
    "    for i in range(0, batch_size * num_batches, batch_size):\n",
    "        # Here, `initial_indices` contains randomized starting indices for\n",
    "        # subsequences\n",
    "        initial_indices_per_batch = initial_indices[i: i + batch_size]\n",
    "        X = [data(j) for j in initial_indices_per_batch]\n",
    "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
    "        yield torch.tensor(X), torch.tensor(Y)\n",
    "        \n",
    "def seq_data_iter_sequential(corpus, batch_size, num_steps):  #@save\n",
    "    \"\"\"Generate a minibatch of subsequences using sequential partitioning.\"\"\"\n",
    "    # Start with a random offset to partition a sequence\n",
    "    offset = random.randint(0, num_steps)\n",
    "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
    "    Xs = torch.tensor(corpus[offset: offset + num_tokens])\n",
    "    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n",
    "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
    "    num_batches = Xs.shape[1] // num_steps\n",
    "    for i in range(0, num_steps * num_batches, num_steps):\n",
    "        X = Xs[:, i: i + num_steps]\n",
    "        Y = Ys[:, i: i + num_steps]\n",
    "        yield X, Y\n",
    "\n",
    "class SeqDataLoader:  #@save\n",
    "    \"\"\"An iterator to load sequence data.\"\"\"\n",
    "    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
    "        if use_random_iter:\n",
    "            self.data_iter_fn = seq_data_iter_random\n",
    "        else:\n",
    "            self.data_iter_fn = seq_data_iter_sequential\n",
    "        self.corpus, self.vocab = load_corpus_time_machine(max_tokens)\n",
    "        self.batch_size, self.num_steps = batch_size, num_steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_time_machine(batch_size, num_steps,  #@save\n",
    "                           use_random_iter=False, max_tokens=10000):\n",
    "    \"\"\"Return the iterator and the vocabulary of the time machine dataset.\"\"\"\n",
    "    data_iter = SeqDataLoader(\n",
    "        batch_size, num_steps, use_random_iter, max_tokens)\n",
    "    return data_iter, data_iter.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "reshape = lambda x, *args, **kwargs: x.reshape(*args, **kwargs)\n",
    "size = lambda x, *args, **kwargs: x.numel(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN-BASIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 28])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = load_data_time_machine(batch_size, num_steps)\n",
    "\n",
    "X = reshape(torch.arange(10), (2, 5))\n",
    "F.one_hot(X.T, 28).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initilise parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(vocab_size, num_hiddens, device):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape, device=device) * 0.01\n",
    "\n",
    "    # Hidden layer parameters\n",
    "    W_xh = normal((num_inputs, num_hiddens))\n",
    "    W_hh = normal((num_hiddens, num_hiddens))\n",
    "    b_h = torch.zeros(num_hiddens, device=device)\n",
    "    # Output layer parameters\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = torch.zeros(num_outputs, device=device)\n",
    "    # Attach gradients\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params\n",
    "\n",
    "def init_rnn_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Hidden and output\n",
    "def rnn(inputs, state, params):\n",
    "    # Here `inputs` shape: (`num_steps`, `batch_size`, `vocab_size`)\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    # Shape of `X`: (`batch_size`, `vocab_size`)\n",
    "    for X in inputs:\n",
    "        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)\n",
    "        Y = torch.mm(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return torch.cat(outputs, dim=0), (H,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModelScratch: #@save\n",
    "    \"\"\"A RNN Model implemented from scratch.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, device,\n",
    "                 get_params, init_state, forward_fn):\n",
    "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
    "        self.params = get_params(vocab_size, num_hiddens, device)\n",
    "        self.init_state, self.forward_fn = init_state, forward_fn\n",
    "\n",
    "    def __call__(self, X, state):\n",
    "        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
    "        return self.forward_fn(X, state, self.params)\n",
    "\n",
    "    def begin_state(self, batch_size, device = device):\n",
    "        return self.init_state(batch_size, self.num_hiddens, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 28]), 1, torch.Size([2, 512]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hiddens = 512\n",
    "net = RNNModelScratch(len(vocab), num_hiddens,device, get_params,\n",
    "                      init_rnn_state, rnn)\n",
    "state = net.begin_state(X.shape[0])\n",
    "Y, new_state = net(X.to(device), state)\n",
    "Y.shape, len(new_state), new_state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ch8(prefix, num_preds, net, vocab, device):  #@save\n",
    "    \"\"\"Generate new characters following the `prefix`.\"\"\"\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "#     print(state)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "#     print(outputs)\n",
    "    get_input = lambda: reshape(torch.tensor(\n",
    "        [outputs[-1]], device=device), (1, 1))\n",
    "#     print(get_input())\n",
    "    for y in prefix[1:]:  # Warm-up period\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "#         print(outputs)\n",
    "    for _ in range(num_preds):  # Predict `num_preds` steps\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am king           '"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ch8('i am king ',10 ,net, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta):  #@save\n",
    "    \"\"\"Clip the gradient.\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n",
    "    \"\"\"Train a net within one epoch (defined in Chapter 8).\"\"\"\n",
    "    state = None\n",
    "    metric = Accumulator(2)  # Sum of training loss, no. of tokens\n",
    "    for X, Y in train_iter:\n",
    "        if state is None or use_random_iter:\n",
    "            # Initialize `state` when either it is the first iteration or\n",
    "            # using random sampling\n",
    "            state = net.begin_state(batch_size=X.shape[0], device=device)\n",
    "        else:\n",
    "            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
    "                # `state` is a tensor for `nn.GRU`\n",
    "                state.detach_()\n",
    "            else:\n",
    "                # `state` is a tuple of tensors for `nn.LSTM` and\n",
    "                # for our custom scratch implementation\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "        y = Y.T.reshape(-1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "#         print(X.shape,y.shape)\n",
    "        y_hat, state = net(X, state)\n",
    "#         print(y_hat.shape)\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            # Since the `mean` function has been invoked\n",
    "            updater(batch_size=1)\n",
    "        metric.add(l * size(y), size(y))\n",
    "    return math.exp(metric[0] / metric[1]), metric[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):\n",
    "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()\n",
    "\n",
    "def train_ch8(net, train_iter, vocab, lr, num_epochs, device,\n",
    "              use_random_iter=False):\n",
    "    \"\"\"Train a model (defined in Chapter 8).\"\"\"\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "#     animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',\n",
    "#                             legend=['train'], xlim=[10, num_epochs])\n",
    "    # Initialize\n",
    "    if isinstance(net, nn.Module):\n",
    "        updater = torch.optim.SGD(net.parameters(), lr)\n",
    "    else:\n",
    "        updater = lambda batch_size: sgd(net.params, lr, batch_size)\n",
    "    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)\n",
    "    # Train and predict\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl, speed = train_epoch_ch8(\n",
    "            net, train_iter, loss, updater, device, use_random_iter)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(predict('time traveller'))\n",
    "#             animator.add(epoch + 1, [ppl])\n",
    "    print(f'perplexity {ppl:.1f}, {speed:.1f} tokens/sec on {str(device)}')\n",
    "    print(predict('time traveller'))\n",
    "    print(predict('traveller'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time traveller and the the that an that an ing the that an that \n",
      "time traveller and and the the this the thime sime sion and and \n",
      "time traveller thing that sime thand the this thithe thas insthe\n",
      "time traveller simentinn sof that lave the thine simension sime \n",
      "time travellere med and are the the chimeno an athe the the mere\n",
      "time travellere mathe atoure beer and and his andimensions ion t\n",
      "time traveller the riment on as hithe t ar alding the giden tome\n",
      "time traveller that is the wion ang stot atole chout of the thon\n",
      "time travelleris tha mathe realle toun and he mat imatile spon t\n",
      "time traveller has a mano thing wiof spect and simats uby cto si\n",
      "time traveller seid filby an t and thay ow mong tere thing an ve\n",
      "time travelleris s at ais the midit yom lanththe and any y an th\n",
      "time travellerit tore aid the reame sund fin thing strecting tha\n",
      "time travellerit s agaigst alo an thathis se atwe dod t at lerst\n",
      "time travellerit tougg tinteot hthe montenges in ais if to us in\n",
      "time travellerio s agsinow y acour thas that ur alat in tof othe\n",
      "time traveller ard ifsemlins the plinthe fimm ef peedinn we bett\n",
      "time travellerit s against reason said fflepsome the gore fowe c\n",
      "time travellerit s against reason said filby bre was ol mowistal\n",
      "time travellerit s against reason said filbycan a cube that does\n",
      "time traveller tre seame rove yeanterthe imatle goth p as feresc\n",
      "time traveller suth thee tore so me pe cepermive ca merece te co\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time traveller woth has ha ssouptig the firy if f ey conguree we\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time travellerit s against reason said ffubysmat ixee tat in atl\n",
      "time traveller sfiled fot gre ore thing fourtitron mertheal in t\n",
      "time travelleryof candinde for end anotyot marnelly stof the bry\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time travellery mon ghilk oust there wise moperpand himede sis l\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "perplexity 1.0, 8960.0 tokens/sec on cuda:0\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "traveller with a slight accession ofcheerfulness really thi\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 500, 1\n",
    "train_ch8(net, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 256\n",
    "rnn_layer = nn.RNN(len(vocab), num_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 512])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (number of hidden layers, batch size, number of hidden units).\n",
    "state = torch.zeros((1, batch_size, num_hiddens))\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([35, 32, 512]), torch.Size([1, 32, 512]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(size=(num_steps, batch_size, len(vocab)))\n",
    "Y, state_new = rnn_layer(X, state)\n",
    "Y.shape, state_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"The RNN model.\"\"\"\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "        # If the RNN is bidirectional (to be introduced later),\n",
    "        # `num_directions` should be 2, else it should be 1.\n",
    "        if not self.rnn.bidirectional:\n",
    "            self.num_directions = 1\n",
    "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "        else:\n",
    "            self.num_directions = 2\n",
    "            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
    "        X = X.to(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        # The fully connected layer will first change the shape of `Y` to\n",
    "        # (`num_steps` * `batch_size`, `num_hiddens`). Its output shape is\n",
    "        # (`num_steps` * `batch_size`, `vocab_size`).\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # `nn.GRU` takes a tensor as hidden state\n",
    "            return  torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens),\n",
    "                                device=device)\n",
    "        else:\n",
    "            # `nn.LSTM` takes a tuple of hidden states\n",
    "            return (torch.zeros((\n",
    "                self.num_directions * self.rnn.num_layers,\n",
    "                batch_size, self.num_hiddens), device=device),\n",
    "                    torch.zeros((\n",
    "                        self.num_directions * self.rnn.num_layers,\n",
    "                        batch_size, self.num_hiddens), device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time travellerttggxxxxxx'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = RNNModel(rnn_layer, vocab_size=len(vocab))\n",
    "net = net.to(device)\n",
    "predict_ch8('time traveller', 10, net, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time traveller the the the the the the the the the the the the t\n",
      "time travellere the the the the the the the the the the the the \n",
      "time traveller and he the the the the the the the the the the th\n",
      "time traveller and and the the the mans an an whime thave the th\n",
      "time traveller the and he pion thing the ghice thing the ghis th\n",
      "time traveller and and the than the thavely thand her aid thas m\n",
      "time traveller sime that sime sion the this this simensing the t\n",
      "time travellerthith sald the the three simensions and the the pe\n",
      "time traveller she cander chions ald the thay ine that is ally h\n",
      "time travellert ar cand the the three dimensions aro the onome t\n",
      "time traveller the gimens of the tire tranell reatly has in s mo\n",
      "time traveller tat fism d arever thisgide tryell mather fice the\n",
      "time travellerit s ag inst yeasl wiy fithe dot ir of her fith do\n",
      "time traveller treccensey by are packlyso lomed is ef are indere\n",
      "time travellerit s against reason s is the or minklas mand antth\n",
      "time traveller alling ou t yome abott in the tiane pss of is wal\n",
      "time travellerittly gaintt ind as ourellyored havifedinsy in ver\n",
      "time travellerit s again t yoo ou t foum the simel for the ot er\n",
      "time traveller ag and meat arald and sard wwy cal teryly insode \n",
      "time traveller fore in time at wellert ine shimg tae aywiyl whav\n",
      "time traveller coment of tter aat init has s anistrint al noper \n",
      "time travellerit wougd be remarkably convenient for the historia\n",
      "time traveller but now you begin to seethe object of my investig\n",
      "time travellerit w ala the e mene a tores yat uading sithinis al\n",
      "time travellerit would be remarkably convenient for the historia\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time travelleryof ch we chal bx inte tryce sone morention to no \n",
      "time traveller with a slight accession ofcheerfulness really thi\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time traveller wath a sereedit come hack hove a chave it thas co\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time travellerit would be remarkably convenient for the historia\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time travellerisus anas thre caldin yor an theravitn tay the tha\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time traveller hatd taithing sis cowuldist of ayour and whicktor\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "perplexity 1.2, 8960.0 tokens/sec on cuda:0\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "travelleryou can show black is white by oral massenead ning\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 500, 1\n",
    "train_ch8(net, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
